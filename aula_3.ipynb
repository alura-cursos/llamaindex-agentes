{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a25f268",
   "metadata": {
    "id": "Ohz6UB2VyBJi"
   },
   "source": [
    "# Aula 1 - Criando um agente de funções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ada0bb",
   "metadata": {
    "id": "3rbpqBGiyG_r"
   },
   "source": [
    "## Vídeo 1.2 - Configurando o LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5d997f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMzMw0wUDiId",
    "outputId": "1a77150f-49a5-4905-aed0-a616f388c852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.12.9-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama_index.embeddings.huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.4.0-py3-none-any.whl.metadata (767 bytes)\n",
      "Collecting llama-index-readers-file\n",
      "  Downloading llama_index_readers_file-0.4.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-llms-groq\n",
      "  Downloading llama_index_llms_groq-0.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting llama-index-tools-tavily-research\n",
      "  Downloading llama_index_tools_tavily_research-0.3.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting crewai\n",
      "  Downloading crewai-0.86.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting crewai-tools\n",
      "  Downloading crewai_tools-0.17.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.9 (from llama-index)\n",
      "  Downloading llama_index_core-0.12.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (0.27.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from llama_index.embeddings.huggingface) (3.3.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (4.12.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (2.2.2)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file)\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-index-llms-openai-like<0.4.0,>=0.3.1 (from llama-index-llms-groq)\n",
      "  Downloading llama_index_llms_openai_like-0.3.3-py3-none-any.whl.metadata (751 bytes)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
      "Collecting tavily-python>=0.2.4 (from llama-index-tools-tavily-research)\n",
      "  Downloading tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting appdirs>=1.4.4 (from crewai)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting auth0-python>=4.7.1 (from crewai)\n",
      "  Downloading auth0_python-4.7.2-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting chromadb>=0.5.18 (from crewai)\n",
      "  Downloading chromadb-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from crewai) (8.1.7)\n",
      "Collecting instructor>=1.3.3 (from crewai)\n",
      "  Downloading instructor-1.7.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting json-repair>=0.25.2 (from crewai)\n",
      "  Downloading json_repair-0.35.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonref>=1.1.0 (from crewai)\n",
      "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting litellm>=1.44.22 (from crewai)\n",
      "  Downloading litellm-1.56.6-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.57.4)\n",
      "Requirement already satisfied: openpyxl>=3.1.5 in /usr/local/lib/python3.10/dist-packages (from crewai) (3.1.5)\n",
      "Requirement already satisfied: opentelemetry-api>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.29.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http>=1.22.0 (from crewai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.29.0)\n",
      "Collecting pdfplumber>=0.11.4 (from crewai)\n",
      "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai) (2.10.3)\n",
      "Collecting python-dotenv>=1.0.0 (from crewai)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pyvis>=0.3.2 (from crewai)\n",
      "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.10/dist-packages (from crewai) (2024.11.6)\n",
      "Collecting tomli-w>=1.1.0 (from crewai)\n",
      "  Downloading tomli_w-1.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: tomli>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from crewai) (2.2.1)\n",
      "Collecting uv>=0.4.25 (from crewai)\n",
      "  Downloading uv-0.5.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting docker>=7.1.0 (from crewai-tools)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting docx2txt>=0.8 (from crewai-tools)\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting embedchain>=0.1.114 (from crewai-tools)\n",
      "  Downloading embedchain-0.1.126-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting lancedb>=0.5.4 (from crewai-tools)\n",
      "  Downloading lancedb-0.17.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting pyright>=1.1.350 (from crewai-tools)\n",
      "  Downloading pyright-1.1.391-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pytest>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from crewai-tools) (8.3.4)\n",
      "Collecting pytube>=15.0.0 (from crewai-tools)\n",
      "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting selenium>=4.18.1 (from crewai-tools)\n",
      "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.5.2 (from gradio)\n",
      "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.45.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
      "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.5 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai) (3.11.10)\n",
      "Requirement already satisfied: cryptography<44.0.0,>=43.0.1 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai) (43.0.3)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai) (2.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
      "Collecting build>=1.0.3 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting posthog>=2.4.0 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.5.18->crewai) (0.21.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.5.18->crewai) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.5.18->crewai) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.5.18->crewai) (1.68.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.5.18->crewai) (9.0.0)\n",
      "Collecting mmh3>=4.0.1 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.5.18->crewai) (13.9.4)\n",
      "Collecting alembic<2.0.0,>=1.13.1 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting chromadb>=0.5.18 (from crewai)\n",
      "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting cohere<6.0,>=5.3 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading cohere-5.13.4-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai-tools) (1.74.0)\n",
      "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai-tools) (0.3.12)\n",
      "Collecting langchain-cohere<0.4.0,>=0.3.0 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading langchain_cohere-0.3.4-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting langchain-community<0.4.0,>=0.3.1 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-openai<0.3.0,>=0.2.1 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting mem0ai<0.2.0,>=0.1.37 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading mem0ai-0.1.38-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting schema<0.8.0,>=0.7.5 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.27 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai-tools) (2.0.36)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.16.1)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai) (0.16)\n",
      "Requirement already satisfied: jiter<0.9,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai) (0.8.2)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai) (2.27.1)\n",
      "Collecting deprecation (from lancedb>=0.5.4->crewai-tools)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "INFO: pip is looking at multiple versions of lancedb to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lancedb>=0.5.4 (from crewai-tools)\n",
      "  Downloading lancedb-0.17.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting pylance==0.20.0 (from lancedb>=0.5.4->crewai-tools)\n",
      "  Downloading pylance-0.20.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pyarrow>=14 in /usr/local/lib/python3.10/dist-packages (from pylance==0.20.0->lancedb>=0.5.4->crewai-tools) (17.0.0)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.22->crewai) (8.5.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.22->crewai) (4.23.0)\n",
      "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.9->llama-index)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.9->llama-index) (1.2.15)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.9->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.9->llama-index)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.9->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.9->llama-index) (3.4.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.9->llama-index)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.9->llama-index) (1.17.0)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.7-py3-none-any.whl.metadata (860 bytes)\n",
      "Collecting openai>=1.13.3 (from crewai)\n",
      "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-groq) (4.47.1)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.13.3->crewai) (1.9.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.5->crewai) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai) (1.66.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.29.0->opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
      "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk>=1.22.0->crewai) (0.50b0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber>=0.11.4->crewai)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber>=0.11.4->crewai)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber>=0.11.4->crewai) (3.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4.2->crewai) (0.7.0)\n",
      "Collecting nodeenv>=1.6.0 (from pyright>=1.1.350->crewai-tools)\n",
      "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai-tools) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai-tools) (1.5.0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai) (7.34.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai) (4.0.1)\n",
      "Collecting trio~=0.17 (from selenium>=4.18.1->crewai-tools)\n",
      "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium>=4.18.1->crewai-tools)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.18.1->crewai-tools) (1.8.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.6.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.13.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.18.3)\n",
      "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb>=0.5.18->crewai)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai) (1.17.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (2.19.2)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (2.27.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (1.25.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (1.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (2.0.6)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache<0.2.0,>=0.1.43->embedchain>=0.1.114->crewai-tools) (5.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.44.22->crewai) (3.21.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (75.1.0)\n",
      "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis>=0.3.2->crewai)\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.48)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.18.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.9.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (0.22.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (1.17.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (3.2.2)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (0.3.3)\n",
      "Collecting langchain-core<0.4.0,>=0.3.25 (from langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-experimental<0.4.0,>=0.3.0 (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (0.9.0)\n",
      "Collecting langchain<0.4.0,>=0.3.1 (from embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.9->llama-index)\n",
      "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->embedchain>=0.1.114->crewai-tools) (1.0.0)\n",
      "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading qdrant_client-1.12.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=0.5.18->crewai)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (24.3.25)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (1.13.1)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai)\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai)\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb>=0.5.18->crewai)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb>=0.5.18->crewai)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb>=0.5.18->crewai) (3.0.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3.0.0,>=2.0.27->embedchain>=0.1.114->crewai-tools) (3.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers<5.0.0,>=4.37.0 (from llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-groq)\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-groq) (0.4.5)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium>=4.18.1->crewai-tools)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium>=4.18.1->crewai-tools)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium>=4.18.1->crewai-tools)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.9->llama-index)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium>=4.18.1->crewai-tools) (1.7.1)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai)\n",
      "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.5.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai) (2.22)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (1.62.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (2.7.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (1.6.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (1.33)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.18->crewai) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading grpcio_tools-1.68.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.18->crewai)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools) (0.6.1)\n",
      "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai-tools)\n",
      "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading llama_index-0.12.9-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.4.0-py3-none-any.whl (8.6 kB)\n",
      "Downloading llama_index_readers_file-0.4.1-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_llms_groq-0.3.1-py3-none-any.whl (2.9 kB)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_tools_tavily_research-0.3.0-py3-none-any.whl (3.3 kB)\n",
      "Downloading crewai-0.86.0-py3-none-any.whl (192 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.0/192.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading crewai_tools-0.17.0-py3-none-any.whl (467 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading auth0_python-4.7.2-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.8/131.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading embedchain-0.1.126-py3-none-any.whl (211 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.4/211.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading instructor-1.7.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading json_repair-0.35.0-py3-none-any.whl (19 kB)\n",
      "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading lancedb-0.17.0-cp39-abi3-manylinux_2_28_x86_64.whl (29.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.9/29.9 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pylance-0.20.0-cp39-abi3-manylinux_2_28_x86_64.whl (33.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading litellm-1.56.6-py3-none-any.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_agent_openai-0.4.1-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.12.9-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_llms_openai_like-0.3.3-py3-none-any.whl (3.1 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.29.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyright-1.1.391-py3-none-any.whl (18 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
      "Downloading tomli_w-1.1.0-py3-none-any.whl (6.4 kB)\n",
      "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading uv-0.5.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading cohere-5.13.4-py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_cohere-0.3.4-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_cloud-0.1.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_parse-0.5.19-py3-none-any.whl (15 kB)\n",
      "Downloading mem0ai-0.1.38-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.3/83.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
      "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Downloading qdrant_client-1.12.2-py3-none-any.whl (267 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.2/267.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading grpcio_tools-1.68.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: docx2txt, pypika, sgmllib3k\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=633b2e4f44b750e0a88a197fd8d762aa4cfee27146593dfa63b8562184002ce4\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=8a7794a7571cb589b98ac7c67fbea1dfcbea2e15f57e01cc713e7ac218f2850c\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=b84c5d99be3ea8401011400874aeda7a6e9414797d0ca20645c79f2d441cf7e1\n",
      "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built docx2txt pypika sgmllib3k\n",
      "Installing collected packages: striprtf, sortedcontainers, sgmllib3k, schema, pypika, pydub, monotonic, filetype, durationpy, docx2txt, dirtyjson, appdirs, wsproto, uvloop, uvicorn, uv, types-requests, tomlkit, tomli-w, semantic-version, ruff, pytube, python-multipart, python-dotenv, pysbd, pyproject_hooks, pypdfium2, pypdf, protobuf, portalocker, parameterized, overrides, outcome, opentelemetry-util-http, nodeenv, mypy-extensions, mmh3, marshmallow, markupsafe, jsonref, json-repair, jedi, hyperframe, humanfriendly, httpx-sse, httptools, hpack, ffmpy, feedparser, fastavro, deprecation, chroma-hnswlib, bcrypt, backoff, asgiref, aiofiles, watchfiles, typing-inspect, trio, tiktoken, starlette, pyright, pylance, posthog, opentelemetry-proto, Mako, httpx, h2, grpcio-tools, gptcache, docker, coloredlogs, build, arxiv, trio-websocket, tokenizers, tavily-python, safehttpx, pyvis, pydantic-settings, pdfminer.six, opentelemetry-exporter-otlp-proto-common, openai, onnxruntime, llama-cloud, langsmith, lancedb, kubernetes, gradio-client, fastapi, dataclasses-json, alembic, transformers, selenium, qdrant-client, pdfplumber, opentelemetry-instrumentation, llama-index-core, litellm, langchain-core, instructor, gradio, cohere, auth0-python, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, mem0ai, llama-parse, llama-index-tools-tavily-research, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, langchain-openai, opentelemetry-instrumentation-fastapi, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-llms-openai-like, llama_index.embeddings.huggingface, llama-index-cli, llama-index-agent-openai, langchain, llama-index-program-openai, llama-index-llms-groq, langchain-community, chromadb, llama-index-question-gen-openai, langchain-experimental, llama-index, langchain-cohere, embedchain, crewai-tools, crewai\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.4\n",
      "    Uninstalling openai-1.57.4:\n",
      "      Successfully uninstalled openai-1.57.4\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.2.3\n",
      "    Uninstalling langsmith-0.2.3:\n",
      "      Successfully uninstalled langsmith-0.2.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.1\n",
      "    Uninstalling transformers-4.47.1:\n",
      "      Successfully uninstalled transformers-4.47.1\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.25\n",
      "    Uninstalling langchain-core-0.3.25:\n",
      "      Successfully uninstalled langchain-core-0.3.25\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.12\n",
      "    Uninstalling langchain-0.3.12:\n",
      "      Successfully uninstalled langchain-0.3.12\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.8 aiofiles-23.2.1 alembic-1.14.0 appdirs-1.4.4 arxiv-2.1.3 asgiref-3.8.1 auth0-python-4.7.2 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 cohere-5.13.4 coloredlogs-15.0.1 crewai-0.86.0 crewai-tools-0.17.0 dataclasses-json-0.6.7 deprecation-2.1.0 dirtyjson-1.0.8 docker-7.1.0 docx2txt-0.8 durationpy-0.9 embedchain-0.1.126 fastapi-0.115.6 fastavro-1.10.0 feedparser-6.0.11 ffmpy-0.5.0 filetype-1.2.0 gptcache-0.1.44 gradio-5.9.1 gradio-client-1.5.2 grpcio-tools-1.68.1 h2-4.1.0 hpack-4.0.0 httptools-0.6.4 httpx-0.27.2 httpx-sse-0.4.0 humanfriendly-10.0 hyperframe-6.0.1 instructor-1.7.2 jedi-0.19.2 json-repair-0.35.0 jsonref-1.1.0 kubernetes-31.0.0 lancedb-0.17.0 langchain-0.3.13 langchain-cohere-0.3.4 langchain-community-0.3.13 langchain-core-0.3.28 langchain-experimental-0.3.4 langchain-openai-0.2.14 langsmith-0.1.147 litellm-1.56.6 llama-cloud-0.1.7 llama-index-0.12.9 llama-index-agent-openai-0.4.1 llama-index-cli-0.4.0 llama-index-core-0.12.9 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-llms-groq-0.3.1 llama-index-llms-openai-0.3.12 llama-index-llms-openai-like-0.3.3 llama-index-multi-modal-llms-openai-0.4.2 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.1 llama-index-readers-llama-parse-0.4.0 llama-index-tools-tavily-research-0.3.0 llama-parse-0.5.19 llama_index.embeddings.huggingface-0.4.0 markupsafe-2.1.5 marshmallow-3.23.2 mem0ai-0.1.38 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 nodeenv-1.9.1 onnxruntime-1.20.1 openai-1.58.1 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-exporter-otlp-proto-http-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-util-http-0.50b0 outcome-1.3.0.post0 overrides-7.7.0 parameterized-0.9.0 pdfminer.six-20231228 pdfplumber-0.11.5 portalocker-2.10.1 posthog-3.7.4 protobuf-5.29.2 pydantic-settings-2.7.1 pydub-0.25.1 pylance-0.20.0 pypdf-5.1.0 pypdfium2-4.30.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyright-1.1.391 pysbd-0.3.4 python-dotenv-1.0.1 python-multipart-0.0.20 pytube-15.0.0 pyvis-0.3.2 qdrant-client-1.12.2 ruff-0.8.5 safehttpx-0.1.6 schema-0.7.7 selenium-4.27.1 semantic-version-2.10.0 sgmllib3k-1.0.0 sortedcontainers-2.4.0 starlette-0.41.3 striprtf-0.0.26 tavily-python-0.5.0 tiktoken-0.7.0 tokenizers-0.20.3 tomli-w-1.1.0 tomlkit-0.13.2 transformers-4.46.3 trio-0.28.0 trio-websocket-0.11.1 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uv-0.5.13 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index llama_index.embeddings.huggingface llama-index-readers-file llama-index-llms-groq arxiv llama-index-tools-tavily-research crewai crewai-tools gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173d2a1c",
   "metadata": {
    "id": "dqHDQ5pGylR0"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd3e624",
   "metadata": {
    "id": "2d8leJQuy1Ig"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "groq = userdata.get('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e08512",
   "metadata": {
    "id": "_p0npmb002xH"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "llm = Groq(model=\"llama-3.3-70b-versatile\", api_key=groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36eb6402",
   "metadata": {
    "id": "g-ZDj1cl3V4l"
   },
   "outputs": [],
   "source": [
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d5d1203",
   "metadata": {
    "id": "GZX5OHSQ3gSD"
   },
   "outputs": [],
   "source": [
    "def calcular_engajamento(curtidas: int, comentarios: int, compartilhamentos: int, seguidores: int) -> str:\n",
    "    \"\"\"\n",
    "    Calcula o engajamento total e a taxa de engajamento de uma postagem.\n",
    "\n",
    "    Args:\n",
    "        curtidas (int): Número de curtidas na postagem.\n",
    "        comentarios (int): Número de comentários na postagem.\n",
    "        compartilhamentos (int): Número de compartilhamentos da postagem.\n",
    "        seguidores (int): Número total de seguidores.\n",
    "\n",
    "    Returns:\n",
    "        dict: Um dicionário contendo o engajamento total (int) e a taxa de engajamento (float, em porcentagem).\n",
    "    \"\"\"\n",
    "    engajamento_total = curtidas + comentarios + compartilhamentos\n",
    "    taxa_engajamento = (engajamento_total / seguidores) * 100 if seguidores > 0 else 0\n",
    "    dicionario = {\"engajamento_total\": engajamento_total,\"taxa_engajamento\": round(taxa_engajamento, 2)}\n",
    "    resultado = f\"O engajamento total é {engajamento_total} e a taxa de engajamento é {round(taxa_engajamento, 2)}%.\"\n",
    "    # Return the dictionary directly\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb8aaa",
   "metadata": {
    "id": "_xzWodeG30vU"
   },
   "source": [
    "## Vídeo 1.3 - Transformando a função em ferramenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff4e4f6",
   "metadata": {
    "id": "SW2TJtS33jgm"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2688d43",
   "metadata": {
    "id": "Pz4gApGf5eI7"
   },
   "outputs": [],
   "source": [
    "ferramenta = FunctionTool.from_defaults(fn=calcular_engajamento,name=\"Calcular Engajamento\",description=(\n",
    "        \"Calcula o engajamento total e a taxa de engajamento de uma postagem nas redes sociais. \"\n",
    "        \"Argumentos: curtidas (int), comentarios (int), compartilhamentos (int), seguidores (int). \"\n",
    "        \"Retorna uma string com o engajamento total e a taxa de engajamento.\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ab7fd8",
   "metadata": {
    "id": "RGZ88sGUInWM"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9efdfb17",
   "metadata": {
    "id": "nAAj1Tq7IpMB"
   },
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[ferramenta],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f77aa1",
   "metadata": {
    "id": "11XOdp_HKDPk"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import AgentRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2de3cfee",
   "metadata": {
    "id": "uOTi0ZqHKLwA"
   },
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd7f193c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8fvWuFsKNqI",
    "outputId": "d22ebadf-40a5-4990-f903-92b664e22e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Qual é o engajamento de uma postagem que teve 150 curtidas, 35 comentários, 20 compartilhamentos, e o perfil tem 2000 seguidores?\n",
      "=== Calling Function ===\n",
      "Calling function: Calcular Engajamento with args: {\"curtidas\": 150, \"comentarios\": 35, \"compartilhamentos\": 20, \"seguidores\": 2000}\n",
      "=== Function Output ===\n",
      "O engajamento total é 205 e a taxa de engajamento é 10.25%.\n",
      "=== LLM Response ===\n",
      "O engajamento total é 205 e a taxa de engajamento é 10.25%.\n"
     ]
    }
   ],
   "source": [
    "response =agent.chat(\"Qual é o engajamento de uma postagem que teve 150 curtidas, 35 comentários, 20 compartilhamentos, e o perfil tem 2000 seguidores?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7756dff1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RxIyuDcKktP",
    "outputId": "151bf0ff-bd4e-4bbb-c6ce-c59b92d8fc73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quem é Albert Einstein?\n",
      "=== LLM Response ===\n",
      "Albert Einstein foi um físico teórico alemão, nascido em 14 de março de 1879, em Munique, Alemanha, e falecido em 18 de abril de 1955, em Princeton, Nova Jersey, EUA. Ele é amplamente considerado um dos cientistas mais influentes do século XX.\n",
      "\n",
      "Einstein é mais conhecido por sua teoria da relatividade, que revolucionou a nossa compreensão do espaço e do tempo. Sua famosa equação E=mc², que relaciona a energia (E) com a massa (m) e a velocidade da luz (c), é uma das mais conhecidas e importantes equações da física.\n",
      "\n",
      "Além disso, Einstein fez contribuições significativas em outras áreas, como a mecânica quântica, a teoria dos campos e a cosmologia. Ele também foi um defensor da paz, da liberdade e da justiça social, e foi um crítico da guerra e da opressão.\n",
      "\n",
      "Einstein recebeu o Prêmio Nobel de Física em 1921 por sua explicação do efeito fotoelétrico, que ajudou a estabelecer a teoria quântica. Ele também foi eleito para a Royal Society em 1921 e recebeu a Medalha Copley em 1925.\n",
      "\n",
      "Hoje em dia, Einstein é considerado um ícone da ciência e da cultura, e sua imagem e ideias continuam a inspirar pessoas em todo o mundo. Sua teoria da relatividade e sua equação E=mc² são consideradas alguns dos maiores feitos da ciência moderna, e sua influência pode ser vista em muitas áreas, desde a física e a engenharia até a filosofia e a arte.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Quem é Albert Einstein?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04aecbb",
   "metadata": {
    "id": "pGVU_4CEK1jv"
   },
   "source": [
    "## Vídeo 1.4 - Consultando artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e879c",
   "metadata": {
    "id": "RQFRh6ywKm74"
   },
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7423a7",
   "metadata": {
    "id": "hVz9_zxvLoPV"
   },
   "outputs": [],
   "source": [
    "def consulta_artigos(titulo: str) -> str:\n",
    "    \"\"\"\n",
    "    Consulta os artigos na base de dados arXiv e retorna resultados formatados.\n",
    "    \"\"\"\n",
    "    busca = arxiv.Search(\n",
    "        query=titulo,\n",
    "        max_results=5,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    resultados = []\n",
    "    for resultado in busca.results():\n",
    "        resultados.append(f\"Título: {resultado.title}\\n\"\n",
    "                          f\"Resumo: {resultado.summary}\\n\"\n",
    "                          f\"Categoria: {resultado.primary_category}\\n\"\n",
    "                          f\"Link: {resultado.entry_id}\\n\")\n",
    "\n",
    "    return \"\\n\\n\".join(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e909405",
   "metadata": {
    "id": "1O49TMAoLo1g"
   },
   "outputs": [],
   "source": [
    "consulta_artigos_tool = FunctionTool.from_defaults(fn=consulta_artigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e73698",
   "metadata": {
    "id": "VKPxtifSLr0D"
   },
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [ferramenta, consulta_artigos_tool],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61656f7",
   "metadata": {
    "id": "NhPXxJcPNXVi"
   },
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9835d",
   "metadata": {
    "id": "Jff1Ax5qNf8g"
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"Me retorne artigos sobre o uso da inteligência artificial nas redes sociais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb1283",
   "metadata": {
    "id": "g6U2VZ3aN2Bc"
   },
   "source": [
    "# Aula 2 - Aprofundando nas pesquisas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b84eda",
   "metadata": {
    "id": "UqNXGikaPNeu"
   },
   "source": [
    "## Vídeo 2.1 - Adotando ferramentas prontas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59ec3c",
   "metadata": {
    "id": "96LLAGw_Nhyt"
   },
   "outputs": [],
   "source": [
    "tavily_key = userdata.get('tavily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3ae41",
   "metadata": {
    "id": "pZKQNeIhQC_L"
   },
   "outputs": [],
   "source": [
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "\n",
    "tavily_tool = TavilyToolSpec(\n",
    "    api_key=tavily_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2455b5a9",
   "metadata": {
    "id": "W_r_uBHZQ0pO"
   },
   "outputs": [],
   "source": [
    "tavily_tool_list = tavily_tool.to_tool_list()\n",
    "for tool in tavily_tool_list:\n",
    "    print(tool.metadata.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf60da9",
   "metadata": {
    "id": "UfGCLEsNRPOJ"
   },
   "outputs": [],
   "source": [
    "tavily_tool.search(\"Me retorne artigos científicos sobre o uso da inteligência artificial nas redes sociais que você encontrar na web\", max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb91d6",
   "metadata": {
    "id": "w7_4Y5x5SL4Q"
   },
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(tavily_tool.to_tool_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50cb051",
   "metadata": {
    "id": "Kb5He2GZSq6Y"
   },
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e76f1",
   "metadata": {
    "id": "dlxN7mQoSr3X"
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"Me retorne artigos científicos sobre o uso da inteligência artificial nas redes sociais que você encontrar na web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32ff14",
   "metadata": {
    "id": "gG_Hi7mES3LP"
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38d8eb",
   "metadata": {
    "id": "hBYZsd43THb_"
   },
   "source": [
    "## Vídeo 2.2 - Criando uma base de dados vetorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb1d36",
   "metadata": {
    "id": "drZHF8AXS5J7"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb0351",
   "metadata": {
    "id": "-iK8Qck35_1Q"
   },
   "outputs": [],
   "source": [
    "url = '/content/drive/MyDrive/Alura/Cursos/4169 - LlamaIndex: Criando Agentes para utilização de ferramentas personalizadas/Projeto/artigo1.pdf'\n",
    "artigo = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0f2e4",
   "metadata": {
    "id": "Xg-8ZcN46CFz"
   },
   "outputs": [],
   "source": [
    "url = '/content/drive/MyDrive/Alura/Cursos/4169 - LlamaIndex: Criando Agentes para utilização de ferramentas personalizadas/Projeto/livro1.pdf'\n",
    "livro = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab31d1",
   "metadata": {
    "id": "jziEJDbr6DOR"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6531e",
   "metadata": {
    "id": "ixXwo7vz6E8h"
   },
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826eb5b",
   "metadata": {
    "id": "tl07WxxP6HAr"
   },
   "outputs": [],
   "source": [
    "artigo_index = VectorStoreIndex.from_documents(artigo)\n",
    "livro_index = VectorStoreIndex.from_documents(livro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b227b2",
   "metadata": {
    "id": "TWMsBn_p6JwH"
   },
   "outputs": [],
   "source": [
    "artigo_index.storage_context.persist(persist_dir=\"/content/drive/MyDrive/Alura/Cursos/4169 - LlamaIndex: Criando Agentes para utilização de ferramentas personalizadas/Projeto/artigo\")\n",
    "livro_index.storage_context.persist(persist_dir=\"/content/drive/MyDrive/Alura/Cursos/4169 - LlamaIndex: Criando Agentes para utilização de ferramentas personalizadas/Projeto/livro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92a7df",
   "metadata": {
    "id": "Pdy88NBw7wg2"
   },
   "source": [
    "## Vídeo 2.3 - Criando engines de busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b05e6",
   "metadata": {
    "id": "P_3cyj-97zkj"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfb7e2",
   "metadata": {
    "id": "wxtSnw6E71ym"
   },
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"/content/drive/MyDrive/Alura/Cursos/4169 - LlamaIndex: Criando Agentes para utilização de ferramentas personalizadas/Projeto/artigo\"\n",
    "    )\n",
    "artigo_index = load_index_from_storage(storage_context)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"/content/drive/MyDrive/Alura/Cursos/4169 - LlamaIndex: Criando Agentes para utilização de ferramentas personalizadas/Projeto/livro\"\n",
    "    )\n",
    "livro_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9ac39",
   "metadata": {
    "id": "hHAidVvc8D5p"
   },
   "outputs": [],
   "source": [
    "artigo_engine = artigo_index.as_query_engine(similarity_top_k=3)\n",
    "livro_engine = livro_index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003ad3a",
   "metadata": {
    "id": "aTHEdBBw8jnz"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fb9fa",
   "metadata": {
    "id": "Foq5aLyM8k9Z"
   },
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=artigo_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"artigo_engine\",\n",
    "            description=(\n",
    "                \"Fornece informações sobre algoritmos de inteligência artificial nas redes sociais. \"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=livro_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"livro_engine\",\n",
    "            description=(\n",
    "                \"Fornece informações sobre avanços e tendências sobre inteligência artificial. \"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb4804",
   "metadata": {
    "id": "fOR226va83AU"
   },
   "source": [
    "# Aula 3 - Verificando o funcionamento do agente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a1a18",
   "metadata": {
    "id": "qXmG2d9BCoWS"
   },
   "source": [
    "## Vídeo 3.1 - Consultando os textos com agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff739a",
   "metadata": {
    "id": "KRREpO_Q9LSp"
   },
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    ")\n",
    "agent_document = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e57f1e",
   "metadata": {
    "id": "-j9jafb3D1sQ"
   },
   "outputs": [],
   "source": [
    "response = agent_document.chat(\n",
    "    \"Quais os principais algoritmos de IA usados nas redes sociais?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a54ad",
   "metadata": {
    "id": "f0mm3D4cEiJL"
   },
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    ")\n",
    "agent_document = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39257707",
   "metadata": {
    "id": "4qIHL-UrEnN9"
   },
   "outputs": [],
   "source": [
    "response = agent_document.chat(\n",
    "    \"Quais as principais tendências de IA que eu deveria estudar?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e031d",
   "metadata": {
    "id": "1-bJe5mREyAM"
   },
   "source": [
    "## Vídeo 3.2 - Usando um agente ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e98f4",
   "metadata": {
    "id": "5IOtpZ00Gl1o"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af5a19",
   "metadata": {
    "id": "1FC1m9oVGv5q"
   },
   "outputs": [],
   "source": [
    "agente = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d59031",
   "metadata": {
    "id": "PNCGGCN8HGBw"
   },
   "outputs": [],
   "source": [
    "response = agente.chat(\"Quais os principais algoritmos de IA usados nas redes sociais?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ec562",
   "metadata": {
    "id": "vZG78rLfHHRh"
   },
   "outputs": [],
   "source": [
    "agente = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47d388",
   "metadata": {
    "id": "06_Lq2uBJMDr"
   },
   "outputs": [],
   "source": [
    "response = agente.chat(\"Quais as principais tendências de IA que eu deveria estudar?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e536a0",
   "metadata": {
    "id": "vxUAqc4dJRmF"
   },
   "outputs": [],
   "source": [
    "agente = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80307134",
   "metadata": {
    "id": "hU_k_pPpJTbl"
   },
   "outputs": [],
   "source": [
    "response = agente.chat(\"Como o feed de uma rede social utiliza IA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d1b4b",
   "metadata": {
    "id": "AZ7rWA6RJbjr"
   },
   "source": [
    "## Vídeo 3.3 - Configurando o CrewAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50985cb2",
   "metadata": {
    "id": "RsBICGY3JjBQ"
   },
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai_tools import LlamaIndexTool\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2495c68",
   "metadata": {
    "id": "sP5mO5moJxwe"
   },
   "outputs": [],
   "source": [
    "ferramenta_artigos = FunctionTool.from_defaults(fn=consulta_artigos, name=\"consulta_artigos\", description=\"Consulta os artigos na base de dados arXiv e retorna resultados formatados.\")\n",
    "tool = LlamaIndexTool.from_tool(ferramenta_artigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527387cc",
   "metadata": {
    "id": "bOx3D1ABNjw0"
   },
   "outputs": [],
   "source": [
    "nvidia = userdata.get('nvidia-llama405b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bd5de",
   "metadata": {
    "id": "VPQi775FMY1w"
   },
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=\"nvidia_nim/meta/llama-3.3-70b-instruct\",\n",
    "    api_key=nvidia\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246331c9",
   "metadata": {
    "id": "IC5yFKdiPk5L"
   },
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    role='Agente de pesquisa',\n",
    "    goal='Fornece artigos científicos sobre um assunto de interesse.',\n",
    "    backstory='Um agente expert em pesquisa científica que possui a habilidade de buscar artigos no arxiv',\n",
    "    tools=[tool],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746da286",
   "metadata": {
    "id": "Dwhc9vtePmqJ"
   },
   "outputs": [],
   "source": [
    "task1 = Task(\n",
    "    description=\"Busque artigos científicos sobre o uso da inteligência artificial nas redes sociais. \",\n",
    "    expected_output=\"5 artigos e seus respectivos links\",\n",
    "    agent=agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2be021",
   "metadata": {
    "id": "6TUs7HLRPtqu"
   },
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "    agents=[agent],\n",
    "    tasks=[task1],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43803f94",
   "metadata": {
    "id": "qD3Gp2GqPv8A"
   },
   "outputs": [],
   "source": [
    "result = crew.kickoff()\n",
    "\n",
    "print(\"######################\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}